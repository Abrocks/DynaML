{"name":"Bayeslearn","tagline":"Scala Library/REPL for scalable Machine Learning","body":"# Bayes Learn\r\n\r\n[![Build Status](https://travis-ci.org/mandar2812/bayeslearn.svg?branch=master)](https://travis-ci.org/mandar2812/bayeslearn)\r\n\r\nAim\r\n============\r\n\r\nBayes Learn is a scala library/repl for implementing and working with general Machine Learning models. Machine Learning/AI applications make heavy use of various entities such as graphs, vectors, matrices etc as well as classes of mathematical models which deal with broadly three kinds of tasks, prediction, classification and clustering.\r\n\r\nThe aim is to build a robust set of abstract classes and interfaces, which can be extended easily to implement advanced models for small and large scale applications.\r\n\r\nBut the library can also be used as an educational/research tool for multi scale data analysis. \r\n\r\nCurrently Bayes Learn has implementations of Least Squares Support Vector Machine (LS-SVM) for binary classification and regression. LS-SVM is equivalent to ridge regression/Tikhonov regularization, for further background consider [Wikipedia](https://en.wikipedia.org/wiki/Least_squares_support_vector_machine) or the [book] (http://www.amazon.com/Least-Squares-Support-Vector-Machines/dp/9812381511).   \r\n\r\nA good general introduction to Probabilistic Models for Machine Learning can be found [here](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/131214.pdf) in [David Barber's](http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.HomePage) text book. The LS-SVM is equivalent to the class of models discussed in Chapter 18 (Bayesian Linear Models) of the book.\r\n\r\nInstallation\r\n============\r\nPrerequisites: Maven\r\n\r\n* Clone this repository\r\n* Run the following.\r\n```shell\r\n  mvn clean compile\r\n  mvn package\r\n```\r\n\r\n* Make sure you give execution permission to `bayeslearn-repl` in the `target/bin` directory.\r\n```shell\r\n  chmod +x target/bin/bayesLearn-repl\r\n  target/bin/bayesLearn-repl\r\n```\r\n  You should get the following prompt.\r\n  \r\n```\r\n       ___           ___           ___           ___           ___              \r\n     /\\  \\         /\\  \\         |\\__\\         /\\  \\         /\\  \\             \r\n    /::\\  \\       /::\\  \\        |:|  |       /::\\  \\       /::\\  \\            \r\n   /:/\\:\\  \\     /:/\\:\\  \\       |:|  |      /:/\\:\\  \\     /:/\\ \\  \\           \r\n  /::\\~\\:\\__\\   /::\\~\\:\\  \\      |:|__|__   /::\\~\\:\\  \\   _\\:\\~\\ \\  \\          \r\n /:/\\:\\ \\:|__| /:/\\:\\ \\:\\__\\     /::::\\__\\ /:/\\:\\ \\:\\__\\ /\\ \\:\\ \\ \\__\\         \r\n \\:\\~\\:\\/:/  / \\/__\\:\\/:/  /    /:/~~/~    \\:\\~\\:\\ \\/__/ \\:\\ \\:\\ \\/__/         \r\n  \\:\\ \\::/  /       \\::/  /    /:/  /       \\:\\ \\:\\__\\    \\:\\ \\:\\__\\           \r\n   \\:\\/:/  /        /:/  /     \\/__/         \\:\\ \\/__/     \\:\\/:/  /           \r\n    \\::/__/        /:/  /                     \\:\\__\\        \\::/  /            \r\n     ~~            \\/__/                       \\/__/         \\/__/             \r\n      ___       ___           ___           ___           ___                  \r\n     /\\__\\     /\\  \\         /\\  \\         /\\  \\         /\\__\\                 \r\n    /:/  /    /::\\  \\       /::\\  \\       /::\\  \\       /::|  |                \r\n   /:/  /    /:/\\:\\  \\     /:/\\:\\  \\     /:/\\:\\  \\     /:|:|  |                \r\n  /:/  /    /::\\~\\:\\  \\   /::\\~\\:\\  \\   /::\\~\\:\\  \\   /:/|:|  |__              \r\n /:/__/    /:/\\:\\ \\:\\__\\ /:/\\:\\ \\:\\__\\ /:/\\:\\ \\:\\__\\ /:/ |:| /\\__\\             \r\n \\:\\  \\    \\:\\~\\:\\ \\/__/ \\/__\\:\\/:/  / \\/_|::\\/:/  / \\/__|:|/:/  /             \r\n  \\:\\  \\    \\:\\ \\:\\__\\        \\::/  /     |:|::/  /      |:/:/  /              \r\n   \\:\\  \\    \\:\\ \\/__/        /:/  /      |:|\\/__/       |::/  /               \r\n    \\:\\__\\    \\:\\__\\         /:/  /       |:|  |         /:/  /                \r\n     \\/__/     \\/__/         \\/__/         \\|__|         \\/__/                 \r\n\r\nWelcome to Bayes Learn v 1.2\r\nInteractive Scala shell\r\nSTADIUS ESAT KU Leuven (2015)\r\n\r\nbayeslearn>\r\n  \r\n```\r\n\r\nGetting Started\r\n===============\r\n\r\nThe `data/` directory contains a few sample data sets, and the root directory also has example scripts which can be executed in the shell.\r\n\r\n* First we create a linear classification model on a csv data set. We will assume that the last column in each line of the file is the target value, and we build an LS-SVM model.\r\n\r\n```scala\r\n\tval config = Map(\"file\" -> \"data/ripley.csv\", \"delim\" -> \",\", \"head\" -> \"false\", \"task\" -> \"classification\")\r\n\tval model = GaussianLinearModel(config)\r\n```\r\n\r\n* We can now (optionally) add a Kernel on the model to create a generalized linear Bayesian model.\r\n\r\n```scala\r\n  val rbf = new RBFKernel(1.025)\r\n  model.applyKernel(rbf)\r\n```\r\n\r\n```\r\n15/06/25 22:30:57 INFO SVMKernel$: Constructing key-value representation of kernel matrix.\r\n15/06/25 22:30:57 INFO SVMKernel$: Dimension: 13 x 13\r\n15/06/25 22:30:57 INFO SVMKernelMatrix: Eigenvalue decomposition of the kernel matrix using JBlas.\r\n15/06/25 22:30:57 INFO SVMKernelMatrix: Eigenvalue stats: 0.09797818213131776 =< lambda =< 3.178218421049352\r\n15/06/25 22:30:57 INFO GaussianLinearModel: Applying Feature map to data set\r\n15/06/25 22:30:57 INFO GaussianLinearModel: DONE: Applying Feature map to data set\r\nbayeslearn>\r\n```\r\n\r\n* Now we can solve the optimization problem posed by the LS-SVM in the parameter space. Since the LS-SVM problem is equivalent to ridge regression, we have to specify a regularization constant.\r\n\r\n```scala\r\n  model.setRegParam(1.5).learn\r\n```\r\n\r\n* We can now predict the value of the target variable given a new point consisting of a Vector of features using `model.predict()`.\r\n\r\n* Evaluating models is easy in Bayes Learn. You can create an evaluation object as follows. \r\n\r\n```scala\r\n\tval configtest = Map(\"file\" -> \"data/ripleytest.csv\", \"delim\" -> \",\", \"head\" -> \"false\")\r\n\tval met = model.evaluate(configtest)\r\n\tmet.print\r\n```\r\n\r\n* The object `met` has a `print()` method which will dump some performance metrics in the shell. But you can also generate plots by using the `generatePlots()` method.\r\n\r\n```\r\n15/06/25 22:35:06 INFO BinaryClassificationMetrics: Classification Model Performance\r\n15/06/25 22:35:06 INFO BinaryClassificationMetrics: ============================\r\n15/06/25 22:35:06 INFO BinaryClassificationMetrics: Area under PR: NaN\r\n15/06/25 22:35:06 INFO BinaryClassificationMetrics: Area under ROC: 0.8160130718954248\r\n```\r\n\r\n```scala\r\nmet.generatePlots\r\n```\r\n\r\n![Image of Plots](https://lh6.googleusercontent.com/Gu5FAbpMth4uztAJznoVRtT7gmwQWRuUM4w8jZ0kUuJKpQTnJros6iVD1I4bqqY6hTGUtMNWOSNlj4c=w1291-h561-rw)\r\n\r\n* Although kernel based models allow great flexibility in modeling non linear behavior in data, they are highly sensitive to the values of their hyper-parameters. For example if we use a Radial Basis Function (RBF) Kernel, it is a non trivial problem to find the best values of the kernel bandwidth and the regularization constant.\r\n\r\n* In order to find the best hyper-parameters for a general kernel based supervised learning model, we use methods in gradient free global optimization. This is relevant because the cost (objective) function for the hyper-parameters is not smooth in general. In fact in most common scenarios the objective function is defined in terms of some kind of cross validation performance.\r\n\r\n* Bayes Learn has a robust global optimization API, currently Coupled Simulated Annealing and Grid Search algorithms are implemented, the API in the package ```org.kuleven.esat.optimization``` can be extended to implement any general gradient or gradient free optimization methods.\r\n\r\n* Lets tune an RBF kernel on the Ripley data.\r\n\r\n```scala\r\nimport com.tinkerpop.blueprints.Graph\r\nimport com.tinkerpop.frames.FramedGraph\r\nimport org.kuleuven.esat.graphUtils.CausalEdge\r\nval (optModel, optConfig) = KernelizedModel.getOptimizedModel[FramedGraph[Graph],\r\n      Iterable[CausalEdge], model.type](model, \"csa\",\r\n      \"RBF\", 13, 7, 0.3, true)\r\n```\r\n\r\nWe see a long list of logs which end in something like the snippet below, the Coupled Simulated Annealing model, gives us a set of hyper-parameters and their values. \r\n```\r\noptModel: org.kuleuven.esat.graphicalModels.GaussianLinearModel = org.kuleuven.esat.graphicalModels.GaussianLinearModel@6adcc6d9\r\noptConfig: scala.collection.immutable.Map[String,Double] = Map(bandwidth -> 4.292522306297284, RegParam -> 7.56099893666852)\r\n```\r\n\r\nTo inspect the performance of this kernel model on an independent test set, we can use the ```model.evaluate()``` function. But before that we must train this 'optimized' kernel model on the training set.\r\n\r\n```scala\r\noptModel.setMaxIterations(2).learn()\r\nval met = optModel.evaluate(configtest)\r\nmet.print()\r\nmet.generatePlots()\r\n```\r\n\r\nAnd the evaluation results follow ...\r\n\r\n```\r\n15/06/25 23:49:32 INFO BinaryClassificationMetrics: Classification Model Performance\r\n15/06/25 23:49:32 INFO BinaryClassificationMetrics: ============================\r\n15/06/25 23:49:32 INFO BinaryClassificationMetrics: Area under PR: NaN\r\n15/06/25 23:49:32 INFO BinaryClassificationMetrics: Area under ROC: 0.8696078431372549\r\n```\r\n\r\nDocumentation\r\n=============\r\nYou can refer to the project [home page](http://mandar2812.github.io/bayeslearn/) or the [documentation](http://mandar2812.github.io/bayeslearn/target/site/scaladocs/index.html#package) for getting started with Bayes Learn. Bear in mind that this is still at its infancy and there will be many more improvements/tweaks in the future.\r\n","google":"UA-59996358-1","note":"Don't delete this file! It's used internally to help with page regeneration."}